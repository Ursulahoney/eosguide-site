name: Daily Scraper - Update All Opportunities

on:
  # Run automatically every day at 6 AM UTC (adjust timezone as needed)
  schedule:
    - cron: '0 6 * * *'
  
  # Also allow manual triggering from GitHub website
  workflow_dispatch:

jobs:
  scrape-and-update:
    runs-on: ubuntu-latest
    
    steps:
      # Step 1: Check out your repository
      - name: Checkout repository
        uses: actions/checkout@v4
      
      # Step 2: Set up Python
      - name: Set up Python 3.11
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      # Step 3: Install dependencies
      - name: Install Python dependencies
        run: |
          cd scrapers
          pip install -r requirements.txt
      
      # Step 4: Run the master scraper
      - name: Run master scraper
        run: |
          cd scrapers
          python master_scraper.py
      
      # Step 5: Move the generated file to data folder
      - name: Move opportunities.json to data folder
        run: |
          mkdir -p data
          cp scrapers/../data/opportunities.json data/opportunities.json || echo "File already in correct location"
      
      # Step 6: Commit and push changes
      - name: Commit updated opportunities
        run: |
          git config user.name "eosguide-bot"
          git config user.email "bot@eosguide.com"
          git add data/opportunities.json
          
          # Only commit if there are changes
          if git diff --staged --quiet; then
            echo "No changes to commit"
          else
            git commit -m "ðŸ¤– Auto-update opportunities - $(date +'%Y-%m-%d %H:%M UTC')"
            git push
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
